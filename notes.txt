# More images per batch -> better feature recognition
# More batches -> better reduction in noise and more accurate phase amplitude
# The above comments assume the total size of the training set is constant, and
# these observations have not been tested over a wide range of conditions.

# Too few connections in the network may also results in loss of contrast. 50000 neurons
# in the hidden layer seems sufficient for 64x64 images (in imaging mode) when using three
# defoci. The same number of neurons results in a blurry image when using only under- and
# over-focus images. Using the two image method with 75000 neurons (an increase of 50%) to
# keep the total number of connections the same did not improve the contrast. Tried
# an increase of 50% in training data, hidden neurons, and batch size (with the same number of
# batches, simulataneously. This improved edge contrast, but not significantly. Trying
# 100000 hidden neurons, with 9000 training sets did not improve the result noticibly beyond
# the previous attempt.
#
# It turns out that the poor contrast can be avoided by setting the mean inner potential (MIP)
# to a smaller value in training data. The resultant output phase is underestimated,
# but the edge contrast is vastly improved. Perhaps the ANN can be trained on data
# with a variety of MIPs, so that it has a better understanding of how the phase is related
# to the MIP.

# Three image method fails to converge for large domain width (relative to defocus?) of 255nm
#(10um defocus). Tested with 'random' initialisation, which also failed to converge. 'zeros'
# converged, but has extremely poor contrast.




# Use a learning rate of approximately 1e-4 for AdamOptimizer, or 0.5 for
GradienDescentOptimizer

